{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0d7cf9",
   "metadata": {},
   "source": [
    "## Reference Implementation\n",
    "\n",
    "### ***E2E Architecture***\n",
    "### **Use Case E2E flow**\n",
    "![Use_case_flow](assets/E2E_2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a68e4",
   "metadata": {},
   "source": [
    "### Solution setup\n",
    "Use the following cell to change to the correct kernel. Then check that you are in the `stock` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:stock]`. Note that the cell will remain with * but you can continue running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-stock-tf-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be99756",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We trained 6 convolution layers and 5 dense layers CNN architecture model to classify the normal and pneumonia from the production pipeline.\n",
    "\n",
    "| **Input Size** | 416x608\n",
    "| :--- | :---\n",
    "| **Output Model format** | TensorFlow checkpoint\n",
    "\n",
    "### Training CNN model\n",
    "\n",
    "**Capturing the time for training**\n",
    "<br>Run the training module as given below to start training and prediction using the active environment. This module takes option to run the training.\n",
    "```\n",
    "usage: medical_diagnosis_initial_training.py  [--datadir] \n",
    "\n",
    "optional arguments:\n",
    "  -h,                   show this help message and exit\n",
    "  \n",
    "  --data_dir \n",
    "                        Absolute path to the data folder containing\n",
    "                        \"chest_xray\" and \"chest_xray\" folder containing \"train\" \"test\" and \"val\" \n",
    "                         and each subfolders contain \"Pneumonia\" and \"NORMAL\" folders \n",
    "```\n",
    "\n",
    "**Command to run training**\n",
    "\n",
    "```sh\n",
    "python src/medical_diagnosis_initial_training.py  --datadir ./data/chest_xray\n",
    "```\n",
    "By default, model checkpoint will be saved in \"model\" folder.\n",
    "\n",
    "> **Note**: If any CV2 dependency comes like \"cv2 import *ImportError: libGL.so.1: cannot open shared object file\" please execute sudo apt install libgl1-mesa-glx apt-get install libgl1 -y pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/medical_diagnosis_initial_training.py --datadir ./data/chest_xray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20876",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter tuning\n",
    "\n",
    " **hyperparameters used here are as below** \n",
    "<br> Dataset remains same with 90:10 split for Training and testing. It needs to be ran multiple times on the same dataset, across different hyper-parameters\n",
    "\n",
    "Below parameters been used for tuning\n",
    "\n",
    "<br>\"learning rates\"      : [0.001, 0.01]\n",
    "<br>\"batchsize\"           : [10 ,20]\n",
    "\n",
    "```\n",
    "usage: medical_diagnosis_hyperparameter_tuning.py \n",
    "\n",
    "optional arguments:\n",
    "  -h,                   show this help message and exit\n",
    "  \n",
    "\n",
    "  --data_dir \n",
    "                        Absolute path to the data folder containing\n",
    "                        \"chest_xray\" and \"chest_xray\" folder containing \"train\" \"test\" and \"val\" \n",
    "                         and each subfolders contain \"Pneumonia\" and \"NORMAL\" folders\n",
    "\n",
    "```\n",
    "**Command to run hyperparameter tuning**\n",
    "\n",
    "```sh\n",
    "python src/medical_diagnosis_hyperparameter_tuning.py   --datadir  ./data/chest_xray\n",
    "```\n",
    "By default, best model checkpoint will be saved in \"model\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/medical_diagnosis_hyperparameter_tuning.py   --datadir  ./data/chest_xray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46747144",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Convert the model to frozen graph**\n",
    "\n",
    "run the conversion module to convert the TensorFlow checkpoint model format to frozen graph format. \n",
    "\n",
    "```\n",
    "usage: python src/model_conversion.py [-h] [--model_dir] [--output_node_names]\n",
    "\n",
    "optional arguments:\n",
    "  -h  \n",
    "                            show this help message and exit\n",
    "  --model_dir\n",
    "                            Please provide the Latest Checkpoint path e.g for\n",
    "                            \"./model\"...Default path is mentioned\n",
    "\n",
    "  --output_node_names       Default path is mentioned as \"Softmax\"\n",
    "```\n",
    "**Command to run conversion**\n",
    "\n",
    "```sh\n",
    "python src/model_conversion.py --model_dir ./model  --output_node_names Softmax\n",
    "```\n",
    ">**Note** : Also we need to generate Stock frozen_graph.pb and move all stock model files in new folder named \"stockmodel\" inside model folder to avoid the overwrite model file conflict when we run scripts in Intel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/model_conversion.py --model_dir ./model  --output_node_names Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235edc8",
   "metadata": {},
   "source": [
    "### 5. Inference\n",
    "\n",
    " Running inference using Stock TensorFlow using 2.8.0 \n",
    "\n",
    "```\n",
    "usage: inference.py [--codebatchsize ] [--modeldir ]\n",
    "\n",
    "optional arguments:\n",
    "  -h,                       show this help message and exit\n",
    "\n",
    "  --codebatchsize           --codebatchsize\n",
    "                              batchsize used for inference\n",
    "                        \n",
    "  --modeldir                --modeldir         \n",
    "                              provide frozen Model path \".pb\" file...users can also\n",
    "                              use INC INT8 quantized model here\n",
    "\n",
    "```\n",
    "**Command to run inference**\n",
    "\n",
    "```sh\n",
    "python src/inference.py --codebatchsize 1  --modeldir ./stockmodel/updated_model.pb\n",
    "```\n",
    ">**Note** : As we mentioned earlier all the stock generated model need to be moved stockmodel folder and codebatchsize can be changed (1,32,64,128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddfbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/inference.py --codebatchsize 1  --modeldir ./model/updated_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be93562",
   "metadata": {},
   "source": [
    "## Optimizing the E2E solution with IntelÂ® oneAPI components\n",
    "\n",
    "### **Use Case E2E flow**\n",
    "\n",
    "![Use_case_flow](assets/E2E_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44385aa",
   "metadata": {},
   "source": [
    "### 1. Environment Creation\n",
    "\n",
    "**Setting up the environment for Intel oneDNN optimized TensorFlow**<br>Follow the below conda installation commands to setup the Intel oneDNN optimized TensorFlow environment for the model training and prediction.\n",
    "```sh\n",
    "conda env create -f env/intel/intel-tf.yml\n",
    "```\n",
    "*Activate intel conda environment*\n",
    "Use the following command to activate the environment that was created:\n",
    "\n",
    "```sh\n",
    "conda activate intel-tf \n",
    "export TF_ENABLE_ONEDNN_OPTS=1\n",
    "export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "```\n",
    ">**Note**: We need to set the above flags everytime before running the scripts below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d04ca",
   "metadata": {},
   "source": [
    "### Solution setup\n",
    "Use the following cell to change to the correct kernel. Then check that you are in the `intel` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:intel]`. Note that the cell will remain with * but you can continue running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-intel-tf-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c50b54",
   "metadata": {},
   "source": [
    "### Training CNN model\n",
    "\n",
    "**Capturing the time for training**\n",
    "<br>Run the training module as given below to start training and prediction using the active environment. This module takes option to run the training.\n",
    "```\n",
    "usage: medical_diagnosis_initial_training.py  [--datadir] \n",
    "\n",
    "optional arguments:\n",
    "  -h,                   show this help message and exit\n",
    "  \n",
    "  --data_dir \n",
    "                        Absolute path to the data folder containing\n",
    "                        \"chest_xray\" and \"chest_xray\" folder containing \"train\" \"test\" and \"val\" \n",
    "                         and each subfolders contain \"Pneumonia\" and \"NORMAL\" folders \n",
    "```\n",
    "**Command to run training**\n",
    "\n",
    "```sh\n",
    "python src/medical_diagnosis_initial_training.py  --datadir ./data/chest_xray\n",
    "```\n",
    "By default, model checkpoint will be saved in \"model\" folder.\n",
    "\n",
    "> **Note**:  If any gcc dependency comes please upgrade it using sudo apt install build-essential.\n",
    "Above training command will run in intel environment and the output trained model would be saved in TensorFlow checkpoint \n",
    "format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70044996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TF_ENABLE_ONEDNN_OPTS=1\n",
    "export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "python src/medical_diagnosis_initial_training.py  --datadir ./data/chest_xray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39fa7b",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "**Hyperparameters used here are as below** \n",
    "<br> Dataset remains same with 90:10 split for Training and testing. It needs to be ran multiple times on the same dataset, across different hyper-parameters\n",
    "\n",
    "Below parameters been used for tuning\n",
    "\n",
    "<br>\"learning rates\"      : [0.001, 0.01]\n",
    "<br>\"batchsize\"           : [10,20]\n",
    "\n",
    "```\n",
    "usage: medical_diagnosis_hyperparameter_tuning.py \n",
    "\n",
    "optional arguments:\n",
    "  -h,                   show this help message and exit\n",
    "  \n",
    "\n",
    "  --data_dir \n",
    "                        Absolute path to the data folder containing\n",
    "                        \"chest_xray\" and \"chest_xray\" folder containing \"train\" \"test\" and \"val\" \n",
    "                         and each subfolders contain \"Pneumonia\" and \"NORMAL\" folders\n",
    "\n",
    "```\n",
    "**Command to run hyperparameter tuning**\n",
    "\n",
    "```sh\n",
    "python src/medical_diagnosis_hyperparameter_tuning.py --datadir  ./data/chest_xray\n",
    "```\n",
    "By default, model checkpoint will be saved in \"model\" folder.\n",
    "\n",
    "> **Note**: Here using --codebatchsize 20 and  --learningRate 0.001 best accuracy has been evaluated ,even that model is compatible for INC conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TF_ENABLE_ONEDNN_OPTS=1\n",
    "export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "python src/medical_diagnosis_hyperparameter_tuning.py --datadir  ./data/chest_xray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37796e2",
   "metadata": {},
   "source": [
    "<br>**Convert the model to frozen graph**\n",
    "\n",
    "Run the conversion module to convert the TensorFlow checkpoint model format to frozen graph format. This frozen graph can be later used for Inferencing, INC and IntelÂ® Distribution of OpenVINOâ¢.\n",
    "```\n",
    "usage: python src/model_conversion.py [-h] [--model_dir] [--output_node_names]\n",
    "\n",
    "optional arguments:\n",
    "  -h  \n",
    "                            show this help message and exit\n",
    "  --model_dir\n",
    "                            Please provide the Latest Checkpoint path e.g for\n",
    "                            \"./model\"...Default path is mentioned\n",
    "\n",
    "  --output_node_names       Default name is mentioned as \"Softmax\"\n",
    "```\n",
    "**Command to run conversion**\n",
    "\n",
    "```sh\n",
    "python src/model_conversion.py --model_dir ./model --output_node_names Softmax\n",
    "```\n",
    "> **Note**: We need to make sure intel frozen_graph.pb gets generated using intel model files only \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/model_conversion.py --model_dir ./model --output_node_names Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d534d",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Performed inferencing on the trained model using TensorFlow  2.9.0 with oneDNN\n",
    "\n",
    "#### Running inference using TensorFlow\n",
    "\n",
    "```\n",
    "usage: inference.py [--codebatchsize ] [--modeldir ]\n",
    "\n",
    "optional arguments:\n",
    "  -h,                       show this help message and exit\n",
    "\n",
    "  --codebatchsize           --codebatchsize\n",
    "                              batchsize used for inference\n",
    "                        \n",
    "  --modeldir                --modeldir         \n",
    "                              provide frozen Model path \".pb\" file...users can also\n",
    "                              use INC INT8 quantized model here\n",
    "\n",
    "```\n",
    "**Command to run inference**\n",
    "\n",
    "```sh\n",
    "OMP_NUM_THREADS=4 KMP_BLOCKTIME=100 python src/inference.py --codebatchsize 1  --modeldir ./model/updated_model.pb\n",
    "```\n",
    ">**Note** : Above inference script can be run in intel environment using different batch sizes<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d695542",
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMP_NUM_THREADS=4 KMP_BLOCKTIME=100 python src/inference.py --codebatchsize 1  --modeldir ./model/updated_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960c617",
   "metadata": {},
   "source": [
    "### Quantize trained models using IntelÂ® Neural Compressor\n",
    "\n",
    "IntelÂ® Neural Compressor is used to quantize the FP32 Model to the INT8 Model. Optimized model is used here for evaluating and timing Analysis.\n",
    "IntelÂ® Neural Compressor supports many optimization methods. In this case, we used post training quantization with `Default Quantiztion Mode` method to quantize the FP32 model.\n",
    "\n",
    ">**Note**: We need to make sure intel frozen_graph.pb gets generated using intel model files only .We recommend initiate running hyperparametertuning script with default parameter to get a new model then convert to Frozen graph and using that get the compressed model , if model gets corrupted for any reason below script will not run .\n",
    "\n",
    "*Step-1: Conversion of FP32 Model to INT8 Model*\n",
    "\n",
    "```\n",
    "usage: src/INC/neural_compressor_conversion.py  [--modelpath] ./model/updated_model.pb  [--outpath] ./model/output/compressedmodel.pb [--config]  ./src/INC/deploy.yaml\n",
    "\n",
    "optional arguments:\n",
    "  -h                          show this help message and exit\n",
    "\n",
    "  --modelpath                 --modelpath \n",
    "                                Model path trained with TensorFlow \".pb\" file\n",
    "  --outpath                   --outpath \n",
    "                                default output quantized model will be save in \".model//output\" folder\n",
    "  --config                    --config \n",
    "                                Yaml file for quantizing model, default is \"./deploy.yaml\"\n",
    "  \n",
    "```\n",
    "\n",
    "**Command to run the neural_compressor_conversion**\n",
    "> Activate intel Environment before running\n",
    "\n",
    "```\n",
    " python src/INC/neural_compressor_conversion.py  --modelpath  ./model/updated_model.pb  --outpath ./model/output/compressedmodel.pb  --config  ./src/INC/deploy.yaml\n",
    "```\n",
    "> Quantized model will be saved by default in `model/output` folder as `compressedmodel.pb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c74e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/INC/neural_compressor_conversion.py  --modelpath  ./model/updated_model.pb  --outpath ./model/output/compressedmodel.pb  --config  ./src/INC/deploy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fffc28",
   "metadata": {},
   "source": [
    "*Step-2: Inferencing using quantized Model*\n",
    "\n",
    "```\n",
    "usage: inference_inc.py [--codebatchsize ] [--modeldir ]\n",
    "\n",
    "optional arguments:\n",
    "  -h,                       show this help message and exit\n",
    "\n",
    "  --codebatchsize           --codebatchsize\n",
    "                              batchsize used for inference\n",
    "                        \n",
    "  --modeldir                --modeldir         \n",
    "                              provide frozen Model path \".pb\" file...users can also\n",
    "                              use INC INT8 quantized model here\n",
    "\n",
    "```\n",
    "**Command to run inference**\n",
    "\n",
    "```sh\n",
    "OMP_NUM_THREADS=4 KMP_BLOCKTIME=100 python src/INC/inference_inc.py --codebatchsize 1  --modeldir ./model/updated_model.pb\n",
    "```\n",
    ">**Note** : Above inference script can be run in intel environment using different batch sizes<br>\n",
    "Same script can be used to benchmark INC INT8 Quantized model. For more details please refer to INC quantization section.By using different batchsize one can observe the gain obtained using IntelÂ® oneDNN optimized TensorFlow in intel environment. <br>\n",
    "\n",
    "Run this script to record multiple trials and the minimum value can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65354c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMP_NUM_THREADS=4 KMP_BLOCKTIME=100 python src/INC/inference_inc.py --codebatchsize 1  --modeldir ./model/updated_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24e0c8",
   "metadata": {},
   "source": [
    "*Step-3 : Performance of  quantized Model*\n",
    "\n",
    "```\n",
    "usage: src/INC/run_inc_quantization_acc.py  [--datapath]   [--fp32modelpath]  [--config]   [--int8modelpath ]\n",
    "\n",
    "optional arguments:\n",
    "  -h,                       show this help message and exit\n",
    "\n",
    "  --datapath                --datapath\n",
    "                              need to mention absolute path of data\n",
    "                        \n",
    "  ---fp32modelpath          --fp32modelpath         \n",
    "                              provide frozen Model path \".pb\" file...(Absolute path)\n",
    "\n",
    "  --config                  --config        \n",
    "                              provide config path...(Absolute path)\n",
    "\n",
    "  --int8modelpath          --int8modelpath      \n",
    "                             provide int8 model path \".pb\" file...(Absolute path)\n",
    "                              \n",
    "\n",
    "```\n",
    "\n",
    "**Command to run Evalution of INT8 Model**\n",
    "\n",
    "```sh\n",
    "python src/INC/run_inc_quantization_acc.py --datapath ./data/chest_xray/val --fp32modelpath ./model/updated_model.pb --config ./src/INC/deploy.yaml --int8modelpath ./model/output/compressedmodel.pb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/INC/run_inc_quantization_acc.py --datapath ./data/chest_xray/val --fp32modelpath ./model/updated_model.pb --config ./src/INC/deploy.yaml --int8modelpath ./model/output/compressedmodel.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ced8a6",
   "metadata": {},
   "source": [
    "### Quantize trained models using  IntelÂ® Distribution of OpenVINOâ¢\n",
    "\n",
    "When it comes to the deployment of this model on edge devices, with less computing and memory resources, we further need to explore options for quantizing and compressing the model which brings out the same level of accuracy and efficient utilization of underlying computing resources. IntelÂ® Distribution of OpenVINOâ¢ Toolkit facilitates the optimization of a deep learning model from a framework and deployment using an inference engine on such computing platforms based on Intel hardware accelerators. Below section covers the steps to use this toolkit for the model quantization and measure its performance.\n",
    "\n",
    "**IntelÂ® Distribution of OpenVINOâ¢ Intermediate Representation (IR) conversion** <br>\n",
    "Below are the steps to convert TensorFlow frozen graph representation to OpenVINO IR using model optimizer.\n",
    "\n",
    "*Environment Setup*\n",
    "\n",
    "IntelÂ® Distribution of OpenVINOâ¢ is installed in OpenVINO environment. Since IntelÂ® Distribution of OpenVINOâ¢ supports Tensorflow<2.6.0.\n",
    "\n",
    "```sh\n",
    "conda env create -f env/OpenVINO.yml\n",
    "```\n",
    "*Activate OpenVINO environment*\n",
    "```sh\n",
    "conda activate OpenVINO\n",
    "```\n",
    "\n",
    "\n",
    "Frozen graph model should be generated using `model_conversion.py`, post training from the trained TensorFlow checkpoint model.\n",
    "\n",
    "**Command to create IntelÂ® Distribution of OpenVINOâ¢ FPIR model**\n",
    "\n",
    "```sh\n",
    "mo --input_meta_graph ./model/Medical_Diagnosis_CNN.meta --input_shape=\"[1,300,300,3]\" --mean_values=\"[127.5,127.5,127.5]\" --scale_values=\"[127.5]\" --data_type FP32 --output_dir ./model  --input=\"Placeholder\" --output=\"Softmax\"\n",
    "```\n",
    "\n",
    ">>**Note**: The above step will generate `Medical_Diagnosis_CNN.bin` and `Medical_Diagnosis_CNN.xml` as output in `model` which can be used with OpenVINO inference application. Default precision is FP32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f412805",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-OpenVINO-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77503e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo --input_meta_graph ./model/Medical_Diagnosis_CNN.meta --input_shape=\"[1,300,300,3]\" --mean_values=\"[127.5,127.5,127.5]\" --scale_values=\"[127.5]\" --data_type FP32 --output_dir ./model  --input=\"Placeholder\" --output=\"Softmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cbc51e",
   "metadata": {},
   "source": [
    "#### Model Quantization\n",
    "\n",
    "```\n",
    "python src/OPENVINO/run_openvino_script.py  --datapath ./data/chest_xray/val  --modelpath ./model/Medical_Diagnosis_CNN.xml\n",
    "\n",
    "optional arguments:\n",
    "  -h,                     show this help message and exit\n",
    "\n",
    "  --modelpath,            --modelpath\n",
    "  \n",
    "  --datapath              --datapath\n",
    "                            dataset folder containing \"val\"\n",
    "      \n",
    "```\n",
    "**Command to run coversion of OpenVINO FPIR model to INT8 model**\n",
    "\n",
    "```sh\n",
    "python src/OPENVINO/run_openvino_script.py  --datapath ./data/chest_xray/val  --modelpath ./model/Medical_Diagnosis_CNN.xml\n",
    "```\n",
    "\n",
    "> The above step will quantize the model and generate `Medical_Diagnosis_CNN.bin` and `Medical_Diagnosis_CNN.xml` as output in `./model/optimized` which can be used with  IntelÂ® Distribution of OpenVINO throughput and latency benchmarking. post quantization precision is INT8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65642e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/OPENVINO/run_openvino_script.py  --datapath ./data/chest_xray/val  --modelpath ./model/Medical_Diagnosis_CNN.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365062fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Benchmarking with  IntelÂ® Distribution of OpenVINOâ¢ Post-Training Optimization Tool\n",
    "\n",
    "**Running inference using IntelÂ® Distribution of OpenVINOâ¢**<br>Command to perform inference using IntelÂ® Distribution of OpenVINOâ¢. The model needs to be converted to IR format as per the section. \n",
    "Post-training Optimization Tool (POT) is designed to accelerate the inference of deep learning models by applying special methods without model retraining or fine-tuning, like post-training quantization.\n",
    "\n",
    "*Pre-requisites*\n",
    "-  IntelÂ® Distribution of OpenVINOâ¢ Toolkit\n",
    "-  IntelÂ® Distribution of OpenVINO IR converted FP32/16 precision model\n",
    "-  IntelÂ® Distribution of OpenVINO INT8 model converted using FPIR model.\n",
    "\n",
    "**Performance Benchmarking of full precision (FP32) Model**<br>Use the below command to run the benchmark tool for the FPIR model generated using this codebase for the Pneumonia detection. \n",
    "\n",
    "```sh\n",
    "Latency mode:\n",
    "benchmark_app -m ./model/Medical_Diagnosis_CNN.xml -api async -niter 120 -nireq 1 -b 1 -nstreams 1 -nthreads 8\n",
    "\n",
    "Throughput mode:\n",
    "benchmark_app -m ./model/Medical_Diagnosis_CNN.xml -api async -niter 120 -nireq 8 -b 32 -nstreams 8 -nthreads 8\n",
    "```\n",
    "\n",
    "**Performance Benchmarking of INT8 precision Model**<br>Use the below command to run the benchmark tool for the quantized INT8 model. \n",
    "\n",
    "```sh\n",
    "Latency mode:\n",
    "benchmark_app -m ./model/optimized/Medical_Diagnosis_CNN.xml  -api async -niter 120 -nireq 1 -b 1 -nstreams 1 -nthreads 8\n",
    "\n",
    "Throughput mode:\n",
    "benchmark_app -m ./model/optimized/Medical_Diagnosis_CNN.xml  -api async -niter 120 -nireq 8 -b 32 -nstreams 8 -nthreads 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m ./model/Medical_Diagnosis_CNN.xml -api async -niter 120 -nireq 1 -b 1 -nstreams 1 -nthreads 8 -hint none\n",
    "!benchmark_app -m ./model/Medical_Diagnosis_CNN.xml -api async -niter 120 -nireq 8 -b 32 -nstreams 8 -nthreads 8 -hint none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m ./model/optimized/Medical_Diagnosis_CNN.xml  -api async -niter 120 -nireq 1 -b 1 -nstreams 1 -nthreads 8 -hint none\n",
    "!benchmark_app -m ./model/optimized/Medical_Diagnosis_CNN.xml  -api async -niter 120 -nireq 8 -b 32 -nstreams 8 -nthreads 8 -hint none"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel-tf]",
   "language": "python",
   "name": "conda-env-intel-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
